{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG19 model pretrained on ImageNet\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "vgg.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content and style layers for style transfer\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = [\n",
    "    'block1_conv1',\n",
    "    'block2_conv1',\n",
    "    'block3_conv1',\n",
    "    'block4_conv1',\n",
    "    'block5_conv1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, (256, 256))\n",
    "    image = image[tf.newaxis, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', tensor, tensor)\n",
    "    input_shape = tf.shape(tensor)\n",
    "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "    return result / num_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_model():\n",
    "    content_model = tf.keras.models.Model(\n",
    "        vgg.input, outputs=[vgg.get_layer(name).output for name in content_layers]\n",
    "    )\n",
    "    return content_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_style_model():\n",
    "    style_model = tf.keras.models.Model(\n",
    "        vgg.input, outputs=[vgg.get_layer(name).output for name in style_layers]\n",
    "    )\n",
    "    return style_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, num_iterations=700, content_weight=1e2, style_weight=30):\n",
    "    content_image = preprocess_image(content_image)\n",
    "    style_image = preprocess_image(style_image)\n",
    "    \n",
    "    content_model = get_content_model()\n",
    "    style_model = get_style_model()\n",
    "\n",
    "    target_content = content_model(content_image)\n",
    "    target_style = style_model(style_image)\n",
    "\n",
    "    generated_image = tf.Variable(content_image)\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.03, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            gen_content = content_model(generated_image)\n",
    "            gen_style = style_model(generated_image)\n",
    "\n",
    "            content_loss = tf.reduce_mean(tf.square(gen_content - target_content))\n",
    "\n",
    "            style_loss = 0\n",
    "            for target_gram, gen_gram in zip(target_style, gen_style):\n",
    "                style_loss += tf.reduce_mean(tf.square(gram_matrix(gen_gram) - gram_matrix(target_gram)))\n",
    "            style_loss /= num_style_layers\n",
    "            \n",
    "            total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, generated_image)\n",
    "        optimizer.apply_gradients([(gradients, generated_image)])\n",
    "        generated_image.assign(clip_0_1(generated_image))\n",
    "        print(f\"Iteration: {iteration + 1}, Total Loss: {total_loss.numpy()}\")\n",
    "\n",
    "    return generated_image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = \"content.png\"\n",
    "style_image_path = \"style.png\"\n",
    "output_image = style_transfer(content_image_path, style_image_path)\n",
    "\n",
    "output_image = np.squeeze(output_image, axis=0)\n",
    "output_image = (output_image * 255).astype(np.uint8)\n",
    "output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "cv2.imshow(\"Cartoonized Image\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite(\"cartoonized_image.jpg\", output_image)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
